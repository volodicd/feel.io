{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T16:01:26.548391Z",
     "start_time": "2024-12-06T16:01:26.533943Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_path = \".\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:01:27.550597Z",
     "start_time": "2024-12-06T16:01:27.513828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the consolidated dataset\n",
    "ravdess_path = os.path.join(root_path, \"audio_speech_actors_01-24\")\n",
    "\n",
    "# Function to parse metadata from filenames\n",
    "def parse_ravdess_metadata(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split(\".\")[0].split(\"-\")\n",
    "    return {\n",
    "        \"file_path\": file_path,\n",
    "        \"modality\": int(parts[0]),  # 01 = full-AV, 02 = video-only, 03 = audio-only\n",
    "        \"vocal_channel\": int(parts[1]),  # 01 = speech, 02 = song\n",
    "        \"emotion\": int(parts[2]),  # Emotion (01 to 08)\n",
    "        \"intensity\": int(parts[3]),  # Intensity (01 = normal, 02 = strong)\n",
    "        \"statement\": int(parts[4]),  # Statement (01 = \"Kids...\", 02 = \"Dogs...\")\n",
    "        \"repetition\": int(parts[5]),  # Repetition (01 = 1st, 02 = 2nd)\n",
    "        \"actor\": int(parts[6]),  # Actor ID (01 to 24)\n",
    "        \"gender\": \"female\" if int(parts[6]) % 2 == 0 else \"male\",  # Gender based on Actor ID\n",
    "    }\n",
    "\n",
    "# Iterate over all actor folders and parse metadata\n",
    "def create_ravdess_annotations(base_path):\n",
    "    rows = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                metadata = parse_ravdess_metadata(file_path)\n",
    "                rows.append(metadata)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Generate annotations\n",
    "ravdess_annotations = create_ravdess_annotations(ravdess_path)\n",
    "\n",
    "# Save annotations to CSV\n",
    "output_csv = \"ravdess_annotations.csv\"\n",
    "ravdess_annotations.to_csv(output_csv, index=False)\n",
    "print(f\"Annotations saved to {output_csv}\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(ravdess_annotations.head())\n"
   ],
   "id": "8e39aefb3ac63a5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations saved to ravdess_annotations.csv\n",
      "                                           file_path  modality  vocal_channel  \\\n",
      "0  ./audio_speech_actors_01-24/Actor_16/03-01-05-...         3              1   \n",
      "1  ./audio_speech_actors_01-24/Actor_16/03-01-06-...         3              1   \n",
      "2  ./audio_speech_actors_01-24/Actor_16/03-01-06-...         3              1   \n",
      "3  ./audio_speech_actors_01-24/Actor_16/03-01-05-...         3              1   \n",
      "4  ./audio_speech_actors_01-24/Actor_16/03-01-07-...         3              1   \n",
      "\n",
      "   emotion  intensity  statement  repetition  actor  gender  \n",
      "0        5          1          2           1     16  female  \n",
      "1        6          1          2           2     16  female  \n",
      "2        6          2          1           2     16  female  \n",
      "3        5          2          1           1     16  female  \n",
      "4        7          1          1           1     16  female  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:01:31.026695Z",
     "start_time": "2024-12-06T16:01:31.005990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "annotations_csv = \"ravdess_annotations.csv\"\n",
    "ravdess_annotations = pd.read_csv(annotations_csv)\n",
    "\n",
    "# Count the number of files per emotion\n",
    "print(\"Emotion distribution:\")\n",
    "print(ravdess_annotations[\"emotion\"].value_counts())\n",
    "\n",
    "# Count the number of files per intensity level\n",
    "print(\"Intensity distribution:\")\n",
    "print(ravdess_annotations[\"intensity\"].value_counts())\n",
    "\n",
    "# Count the number of files per gender\n",
    "print(\"Gender distribution:\")\n",
    "print(ravdess_annotations[\"gender\"].value_counts())\n"
   ],
   "id": "e12af1352e96c98b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion distribution:\n",
      "emotion\n",
      "5    192\n",
      "6    192\n",
      "7    192\n",
      "4    192\n",
      "8    192\n",
      "3    192\n",
      "2    192\n",
      "1     96\n",
      "Name: count, dtype: int64\n",
      "Intensity distribution:\n",
      "intensity\n",
      "1    768\n",
      "2    672\n",
      "Name: count, dtype: int64\n",
      "Gender distribution:\n",
      "gender\n",
      "female    720\n",
      "male      720\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:01:42.467714Z",
     "start_time": "2024-12-06T16:01:33.501496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "normalized_path = \"normalized_audio\"\n",
    "os.makedirs(normalized_path, exist_ok=True)\n",
    "# Define function to normalize audio\n",
    "def normalize_audio(file_path, output_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "        y_normalized = librosa.util.normalize(y)  # Normalize amplitude\n",
    "        sf.write(output_path, y_normalized, sr)  # Save normalized audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Normalize all audio files with a progress bar\n",
    "print(\"Starting audio normalization...\")\n",
    "for _, row in tqdm(ravdess_annotations.iterrows(), total=len(ravdess_annotations), desc=\"Normalizing audio\"):\n",
    "    src = row[\"file_path\"]\n",
    "    dest = os.path.join(normalized_path, os.path.basename(src))\n",
    "    normalize_audio(src, dest)\n",
    "\n",
    "print(\"Audio normalization completed.\")"
   ],
   "id": "c8b937627d9fd2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio normalization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing audio: 100%|██████████| 1440/1440 [00:08<00:00, 161.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio normalization completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:02.035568Z",
     "start_time": "2024-12-06T16:01:57.765255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resample_audio(file_path, output_path, target_sr=16000):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "    sf.write(output_path, y_resampled, target_sr)\n",
    "\n",
    "# Create a directory for resampled audio\n",
    "resampled_path = os.path.join(root_path, \"resampled_audio\")\n",
    "os.makedirs(resampled_path, exist_ok=True)\n",
    "\n",
    "# Resample all audio files\n",
    "for _, row in ravdess_annotations.iterrows():\n",
    "    src = row[\"file_path\"]\n",
    "    dest = os.path.join(resampled_path, os.path.basename(src))\n",
    "    resample_audio(src, dest)\n"
   ],
   "id": "6f1d7d5c05197709",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:19.262632Z",
     "start_time": "2024-12-06T16:02:19.120199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_mel_spectrogram(file_path, n_mels=128, max_length=150):\n",
    "    \"\"\"\n",
    "    Extract Mel Spectrogram and ensure consistent time dimension.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)  # Load audio\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)  # Extract Mel Spectrogram\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to decibel scale\n",
    "\n",
    "        # Pad or truncate to ensure consistent time dimension\n",
    "        if mel_spec_db.shape[1] < max_length:\n",
    "            # Pad with zeros\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, max_length - mel_spec_db.shape[1])), mode='constant')\n",
    "        else:\n",
    "            # Truncate\n",
    "            mel_spec_db = mel_spec_db[:, :max_length]\n",
    "\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example: Extract Mel Spectrogram for the first file\n",
    "example_file = ravdess_annotations[\"file_path\"].iloc[0]\n",
    "mel_spec = extract_mel_spectrogram(example_file)\n",
    "print(\"Mel Spectrogram shape:\", mel_spec.shape)"
   ],
   "id": "b499f396fa28f44a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel Spectrogram shape: (128, 150)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:21.482341Z",
     "start_time": "2024-12-06T16:02:21.220141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    features = [torch.tensor(f, dtype=torch.float32) for f in features]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # Pad features to the same size\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0)\n",
    "    return features_padded, labels\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class RAVDESSDataset(Dataset):\n",
    "    def __init__(self, annotations, feature_extraction_fn):\n",
    "        self.annotations = annotations\n",
    "        self.feature_extraction_fn = feature_extraction_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        file_path = row[\"file_path\"]\n",
    "        label = row[\"emotion\"] - 1  # Adjust labels to start from 0\n",
    "        features = self.feature_extraction_fn(file_path)\n",
    "        return features, label\n",
    "\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = RAVDESSDataset(annotations=ravdess_annotations, feature_extraction_fn=extract_mel_spectrogram)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for features, labels in dataloader:\n",
    "    print(f\"Features shape: {features.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "print(ravdess_annotations[\"emotion\"].value_counts())"
   ],
   "id": "3fa6db12457ee541",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([32, 128, 150]), Labels shape: torch.Size([32])\n",
      "emotion\n",
      "5    192\n",
      "6    192\n",
      "7    192\n",
      "4    192\n",
      "8    192\n",
      "3    192\n",
      "2    192\n",
      "1     96\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:05:27.161017Z",
     "start_time": "2024-12-06T16:02:24.443519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the RNN Model\n",
    "class EmotionRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(EmotionRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=3, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_size, time_steps)\n",
    "        x = x.permute(0, 2, 1)  # Rearrange to (batch_size, time_steps, input_size)\n",
    "        _, (hidden, _) = self.lstm(x)  # hidden: (num_layers * 2, batch_size, hidden_size)\n",
    "        hidden_concat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # Concatenate forward and backward\n",
    "        out = self.fc(self.dropout(hidden_concat))  # Apply dropout before the fully connected layer\n",
    "        return out\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # Mel bands\n",
    "hidden_size = 128  # LSTM hidden layer size\n",
    "num_classes = len(ravdess_annotations[\"emotion\"].unique())  # Number of emotion classes\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model\n",
    "model = EmotionRNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "class_counts = ravdess_annotations[\"emotion\"].value_counts()\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights.values, dtype=torch.float32))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move data to device (if using GPU)\n",
    "        features, labels = features, labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ],
   "id": "126b5fb189f39bb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 36/36 [00:20<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 36/36 [00:22<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 36/36 [00:14<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.9078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 36/36 [00:14<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 1.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 36/36 [00:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 1.8852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 36/36 [00:15<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 1.9086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 36/36 [00:15<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 1.8616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 36/36 [00:23<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 1.8506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 36/36 [00:17<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 1.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 36/36 [00:19<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 1.8338\n",
      "Test Accuracy: 21.88%\n",
      "[[ 0  2  0  0  0  0  0 22]\n",
      " [ 0  5  0  0  0  0  9 14]\n",
      " [ 0  4  0  0  2  0  2 29]\n",
      " [ 0  6  0  0  4  0  3 33]\n",
      " [ 0  2  0  0 12  0  5 16]\n",
      " [ 0  2  0  0  5  0  3 34]\n",
      " [ 0  4  0  0  6  0 12 16]\n",
      " [ 0  1  0  0  0  0  1 34]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.19      0.18      0.19        28\n",
      "           2       0.00      0.00      0.00        37\n",
      "           3       0.00      0.00      0.00        46\n",
      "           4       0.41      0.34      0.38        35\n",
      "           5       0.00      0.00      0.00        44\n",
      "           6       0.34      0.32      0.33        38\n",
      "           7       0.17      0.94      0.29        36\n",
      "\n",
      "    accuracy                           0.22       288\n",
      "   macro avg       0.14      0.22      0.15       288\n",
      "weighted avg       0.14      0.22      0.14       288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volodic/PycharmProjects/feel.io/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/volodic/PycharmProjects/feel.io/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/volodic/PycharmProjects/feel.io/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
